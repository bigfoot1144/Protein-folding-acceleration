{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bda963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from data import Alphabet\n",
    "from categorical_mixture import categorical_lddt\n",
    "from trunk import FoldingTrunk, FoldingTrunkConfig\n",
    "import residue_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8930b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "def _calculate_bin_centers(boundaries: torch.Tensor):\n",
    "    step = boundaries[1] - boundaries[0]\n",
    "    bin_centers = boundaries + step / 2\n",
    "    bin_centers = torch.cat(\n",
    "        [bin_centers, (bin_centers[-1] + step).unsqueeze(-1)], dim=0\n",
    "    )\n",
    "    return bin_centers\n",
    "\n",
    "def _calculate_expected_aligned_error(\n",
    "    alignment_confidence_breaks: torch.Tensor,\n",
    "    aligned_distance_error_probs: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    bin_centers = _calculate_bin_centers(alignment_confidence_breaks)\n",
    "    return (\n",
    "        torch.sum(aligned_distance_error_probs * bin_centers, dim=-1),\n",
    "        bin_centers[-1],\n",
    "    )\n",
    "\n",
    "def compute_tm(\n",
    "    logits: torch.Tensor,\n",
    "    residue_weights: Optional[torch.Tensor] = None,\n",
    "    max_bin: int = 31,\n",
    "    no_bins: int = 64,\n",
    "    eps: float = 1e-8,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    if residue_weights is None:\n",
    "        residue_weights = logits.new_ones(logits.shape[-2])\n",
    "\n",
    "    boundaries = torch.linspace(\n",
    "        0, max_bin, steps=(no_bins - 1), device=logits.device\n",
    "    )\n",
    "\n",
    "    bin_centers = _calculate_bin_centers(boundaries)\n",
    "    torch.sum(residue_weights)\n",
    "    n = logits.shape[-2]\n",
    "    clipped_n = max(n, 19)\n",
    "\n",
    "    d0 = 1.24 * (clipped_n - 15) ** (1.0 / 3) - 1.8\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    tm_per_bin = 1.0 / (1 + (bin_centers ** 2) / (d0 ** 2))\n",
    "    predicted_tm_term = torch.sum(probs * tm_per_bin, dim=-1)\n",
    "\n",
    "    normed_residue_mask = residue_weights / (eps + residue_weights.sum())\n",
    "    per_alignment = torch.sum(predicted_tm_term * normed_residue_mask, dim=-1)\n",
    "\n",
    "    weighted = per_alignment * residue_weights\n",
    "     \n",
    "    argmax = (weighted == torch.max(weighted)).nonzero()[0]\n",
    "    return per_alignment[tuple(argmax)]\n",
    "\n",
    "def compute_predicted_aligned_error(\n",
    "    logits: torch.Tensor,\n",
    "    max_bin: int = 31,\n",
    "    no_bins: int = 64,\n",
    "    **kwargs,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Computes aligned confidence metrics from logits.\n",
    "\n",
    "    Args:\n",
    "      logits: [*, num_res, num_res, num_bins] the logits output from\n",
    "        PredictedAlignedErrorHead.\n",
    "      max_bin: Maximum bin value\n",
    "      no_bins: Number of bins\n",
    "    Returns:\n",
    "      aligned_confidence_probs: [*, num_res, num_res, num_bins] the predicted\n",
    "        aligned error probabilities over bins for each residue pair.\n",
    "      predicted_aligned_error: [*, num_res, num_res] the expected aligned distance\n",
    "        error for each pair of residues.\n",
    "      max_predicted_aligned_error: [*] the maximum predicted error possible.\n",
    "    \"\"\"\n",
    "    boundaries = torch.linspace(\n",
    "        0, max_bin, steps=(no_bins - 1), device=logits.device\n",
    "    )\n",
    "\n",
    "    aligned_confidence_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    (\n",
    "        predicted_aligned_error,\n",
    "        max_predicted_aligned_error,\n",
    "    ) = _calculate_expected_aligned_error(\n",
    "        alignment_confidence_breaks=boundaries,\n",
    "        aligned_distance_error_probs=aligned_confidence_probs,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"aligned_confidence_probs\": aligned_confidence_probs,\n",
    "        \"predicted_aligned_error\": predicted_aligned_error,\n",
    "        \"max_predicted_aligned_error\": max_predicted_aligned_error,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a722731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import residue_constants as rc\n",
    "\n",
    "def make_atom14_masks(protein):\n",
    "    \"\"\"Construct denser atom positions (14 dimensions instead of 37).\"\"\"\n",
    "    restype_atom14_to_atom37 = []\n",
    "    restype_atom37_to_atom14 = []\n",
    "    restype_atom14_mask = []\n",
    "\n",
    "    for rt in rc.restypes:\n",
    "        atom_names = rc.restype_name_to_atom14_names[rc.restype_1to3[rt]]\n",
    "        restype_atom14_to_atom37.append(\n",
    "            [(rc.atom_order[name] if name else 0) for name in atom_names]\n",
    "        )\n",
    "        atom_name_to_idx14 = {name: i for i, name in enumerate(atom_names)}\n",
    "        restype_atom37_to_atom14.append(\n",
    "            [\n",
    "                (atom_name_to_idx14[name] if name in atom_name_to_idx14 else 0)\n",
    "                for name in rc.atom_types\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        restype_atom14_mask.append(\n",
    "            [(1.0 if name else 0.0) for name in atom_names]\n",
    "        )\n",
    "\n",
    "    # Add dummy mapping for restype 'UNK'\n",
    "    restype_atom14_to_atom37.append([0] * 14)\n",
    "    restype_atom37_to_atom14.append([0] * 37)\n",
    "    restype_atom14_mask.append([0.0] * 14)\n",
    "\n",
    "    restype_atom14_to_atom37 = torch.tensor(\n",
    "        restype_atom14_to_atom37,\n",
    "        dtype=torch.int32,\n",
    "        device=protein[\"aatype\"].device,\n",
    "    )\n",
    "    restype_atom37_to_atom14 = torch.tensor(\n",
    "        restype_atom37_to_atom14,\n",
    "        dtype=torch.int32,\n",
    "        device=protein[\"aatype\"].device,\n",
    "    )\n",
    "    restype_atom14_mask = torch.tensor(\n",
    "        restype_atom14_mask,\n",
    "        dtype=torch.float32,\n",
    "        device=protein[\"aatype\"].device,\n",
    "    )\n",
    "    protein_aatype = protein['aatype'].to(torch.long)\n",
    "\n",
    "    # create the mapping for (residx, atom14) --> atom37, i.e. an array\n",
    "    # with shape (num_res, 14) containing the atom37 indices for this protein\n",
    "    residx_atom14_to_atom37 = restype_atom14_to_atom37[protein_aatype]\n",
    "    residx_atom14_mask = restype_atom14_mask[protein_aatype]\n",
    "\n",
    "    protein[\"atom14_atom_exists\"] = residx_atom14_mask\n",
    "    protein[\"residx_atom14_to_atom37\"] = residx_atom14_to_atom37.long()\n",
    "\n",
    "    # create the gather indices for mapping back\n",
    "    residx_atom37_to_atom14 = restype_atom37_to_atom14[protein_aatype]\n",
    "    protein[\"residx_atom37_to_atom14\"] = residx_atom37_to_atom14.long()\n",
    "\n",
    "    # create the corresponding mask\n",
    "    restype_atom37_mask = torch.zeros(\n",
    "        [21, 37], dtype=torch.float32, device=protein[\"aatype\"].device\n",
    "    )\n",
    "    for restype, restype_letter in enumerate(rc.restypes):\n",
    "        restype_name = rc.restype_1to3[restype_letter]\n",
    "        atom_names = rc.residue_atoms[restype_name]\n",
    "        for atom_name in atom_names:\n",
    "            atom_type = rc.atom_order[atom_name]\n",
    "            restype_atom37_mask[restype, atom_type] = 1\n",
    "\n",
    "    residx_atom37_mask = restype_atom37_mask[protein_aatype]\n",
    "    protein[\"atom37_atom_exists\"] = residx_atom37_mask\n",
    "\n",
    "    return protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf2f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm2 import ESM2\n",
    "import re\n",
    "\n",
    "def _load_model_and_alphabet_core_v2(model_data):\n",
    "    def upgrade_state_dict(state_dict):\n",
    "        \"\"\"Removes prefixes 'model.encoder.sentence_encoder.' and 'model.encoder.'.\"\"\"\n",
    "        prefixes = [\"encoder.sentence_encoder.\", \"encoder.\"]\n",
    "        pattern = re.compile(\"^\" + \"|\".join(prefixes))\n",
    "        state_dict = {pattern.sub(\"\", name): param for name, param in state_dict.items()}\n",
    "        return state_dict\n",
    "\n",
    "    cfg = model_data[\"cfg\"][\"model\"]\n",
    "    state_dict = model_data[\"model\"]\n",
    "    state_dict = upgrade_state_dict(state_dict)\n",
    "    alphabet = Alphabet.from_architecture(\"ESM-1b\")\n",
    "    model = ESM2(\n",
    "        num_layers=cfg.encoder_layers,\n",
    "        embed_dim=cfg.encoder_embed_dim,\n",
    "        attention_heads=cfg.encoder_attention_heads,\n",
    "        alphabet=alphabet,\n",
    "        token_dropout=cfg.token_dropout,\n",
    "    )\n",
    "    return model, alphabet, state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baff53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "import typing as T\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from data import Alphabet\n",
    "from categorical_mixture import categorical_lddt\n",
    "from trunk import FoldingTrunk, FoldingTrunkConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ESMFoldConfig:\n",
    "    trunk: T.Any = FoldingTrunkConfig\n",
    "    lddt_head_hid_dim: int = 128\n",
    "\n",
    "class ESMFold(nn.Module):\n",
    "    def __init__(self, esm, esm_dict,  esmfold_config=None, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = esmfold_config if esmfold_config else ESMFoldConfig(**kwargs)\n",
    "        cfg = self.cfg\n",
    "\n",
    "        self.distogram_bins = 64\n",
    "\n",
    "        self.esm = esm\n",
    "        self.esm_dict = esm_dict \n",
    "\n",
    "        self.esm.requires_grad_(False)\n",
    "        self.esm.half()\n",
    "\n",
    "        self.esm_feats = self.esm.embed_dim\n",
    "        self.esm_attns = self.esm.num_layers * self.esm.attention_heads\n",
    "        self.register_buffer(\"af2_to_esm\", ESMFold._af2_to_esm(self.esm_dict))\n",
    "        self.esm_s_combine = nn.Parameter(torch.zeros(self.esm.num_layers + 1))\n",
    "\n",
    "        c_s = cfg.trunk.sequence_state_dim\n",
    "        c_z = cfg.trunk.pairwise_state_dim\n",
    "\n",
    "        self.esm_s_mlp = nn.Sequential(\n",
    "            LayerNorm(self.esm_feats),\n",
    "            nn.Linear(self.esm_feats, c_s),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(c_s, c_s),\n",
    "        )\n",
    "        if cfg.use_esm_attn_map:\n",
    "            self.esm_z_mlp = nn.Sequential(\n",
    "                LayerNorm(self.esm_attns),\n",
    "                nn.Linear(self.esm_attns, c_z),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(c_z, c_z),\n",
    "            )\n",
    "\n",
    "        # 0 is padding, N is unknown residues, N + 1 is mask.\n",
    "        self.n_tokens_embed = residue_constants.restype_num + 3\n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = self.n_tokens_embed - 2\n",
    "        self.mask_idx = self.n_tokens_embed - 1\n",
    "        self.embedding = nn.Embedding(self.n_tokens_embed, c_s, padding_idx=0)\n",
    "\n",
    "        self.trunk = FoldingTrunk(**cfg.trunk)\n",
    "\n",
    "        self.distogram_head = nn.Linear(c_z, self.distogram_bins)\n",
    "        self.ptm_head = nn.Linear(c_z, self.distogram_bins)\n",
    "        self.lm_head = nn.Linear(c_s, self.n_tokens_embed)\n",
    "        self.lddt_bins = 50\n",
    "        self.lddt_head = nn.Sequential(\n",
    "            nn.LayerNorm(cfg.trunk.structure_module.c_s),\n",
    "            nn.Linear(cfg.trunk.structure_module.c_s, cfg.lddt_head_hid_dim),\n",
    "            nn.Linear(cfg.lddt_head_hid_dim, cfg.lddt_head_hid_dim),\n",
    "            nn.Linear(cfg.lddt_head_hid_dim, 37 * self.lddt_bins),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _af2_to_esm(d: Alphabet):\n",
    "        # Remember that t is shifted from residue_constants by 1 (0 is padding).\n",
    "        esm_reorder = [d.padding_idx] + [\n",
    "            d.get_idx(v) for v in residue_constants.restypes_with_x\n",
    "        ]\n",
    "        return torch.tensor(esm_reorder)\n",
    "\n",
    "    def _af2_idx_to_esm_idx(self, aa, mask):\n",
    "        aa = (aa + 1).masked_fill(mask != 1, 0)\n",
    "        return self.af2_to_esm[aa]\n",
    "\n",
    "    def _compute_language_model_representations(\n",
    "        self, esmaa: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Adds bos/eos tokens for the language model, since the structure module doesn't use these.\"\"\"\n",
    "        batch_size = esmaa.size(0)\n",
    "\n",
    "        bosi, eosi = self.esm_dict.cls_idx, self.esm_dict.eos_idx\n",
    "        bos = esmaa.new_full((batch_size, 1), bosi)\n",
    "        eos = esmaa.new_full((batch_size, 1), self.esm_dict.padding_idx)\n",
    "        esmaa = torch.cat([bos, esmaa, eos], dim=1)\n",
    "        # Use the first padding index as eos during inference.\n",
    "        esmaa[range(batch_size), (esmaa != 1).sum(1)] = eosi\n",
    "\n",
    "        res = self.esm(\n",
    "            esmaa,\n",
    "            repr_layers=range(self.esm.num_layers + 1),\n",
    "            need_head_weights=self.cfg.use_esm_attn_map,\n",
    "        )\n",
    "        esm_s = torch.stack(\n",
    "            [v for _, v in sorted(res[\"representations\"].items())], dim=2\n",
    "        )\n",
    "        esm_s = esm_s[:, 1:-1]  # B, L, nLayers, C\n",
    "        esm_z = (\n",
    "            res[\"attentions\"].permute(0, 4, 3, 1, 2).flatten(3, 4)[:, 1:-1, 1:-1, :]\n",
    "            if self.cfg.use_esm_attn_map\n",
    "            else None\n",
    "        )\n",
    "        return esm_s, esm_z\n",
    "\n",
    "    def _mask_inputs_to_esm(self, esmaa, pattern):\n",
    "        new_esmaa = esmaa.clone()\n",
    "        new_esmaa[pattern == 1] = self.esm_dict.mask_idx\n",
    "        return new_esmaa\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        aa: torch.Tensor,\n",
    "        mask: T.Optional[torch.Tensor] = None,\n",
    "        residx: T.Optional[torch.Tensor] = None,\n",
    "        masking_pattern: T.Optional[torch.Tensor] = None,\n",
    "        num_recycles: T.Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Runs a forward pass given input tokens. Use `model.infer` to\n",
    "        run inference from a sequence.\n",
    "\n",
    "        Args:\n",
    "            aa (torch.Tensor): Tensor containing indices corresponding to amino acids. Indices match\n",
    "                openfold.np.residue_constants.restype_order_with_x.\n",
    "            mask (torch.Tensor): Binary tensor with 1 meaning position is unmasked and 0 meaning position is masked.\n",
    "            residx (torch.Tensor): Residue indices of amino acids. Will assume contiguous if not provided.\n",
    "            masking_pattern (torch.Tensor): Optional masking to pass to the input. Binary tensor of the same size\n",
    "                as `aa`. Positions with 1 will be masked. ESMFold sometimes produces different samples when\n",
    "                different masks are provided.\n",
    "            num_recycles (int): How many recycle iterations to perform. If None, defaults to training max\n",
    "                recycles, which is 3.\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(aa)\n",
    "\n",
    "        B = aa.shape[0]\n",
    "        L = aa.shape[1]\n",
    "        device = aa.device\n",
    "\n",
    "        if residx is None:\n",
    "            residx = torch.arange(L, device=device).expand_as(aa)\n",
    "\n",
    "        # === ESM ===\n",
    "        esmaa = self._af2_idx_to_esm_idx(aa, mask)\n",
    "\n",
    "        if masking_pattern is not None:\n",
    "            esmaa = self._mask_inputs_to_esm(esmaa, masking_pattern)\n",
    "\n",
    "        esm_s, esm_z = self._compute_language_model_representations(esmaa)\n",
    "\n",
    "        # Convert esm_s to the precision used by the trunk and\n",
    "        # the structure module. These tensors may be a lower precision if, for example,\n",
    "        # we're running the language model in fp16 precision.\n",
    "        esm_s = esm_s.to(self.esm_s_combine.dtype)\n",
    "        esm_s = esm_s.detach()\n",
    "\n",
    "        # === preprocessing ===\n",
    "        esm_s = (self.esm_s_combine.softmax(0).unsqueeze(0) @ esm_s).squeeze(2)\n",
    "\n",
    "        s_s_0 = self.esm_s_mlp(esm_s)\n",
    "        if self.cfg.use_esm_attn_map:\n",
    "            esm_z = esm_z.to(self.esm_s_combine.dtype)\n",
    "            esm_z = esm_z.detach()\n",
    "            s_z_0 = self.esm_z_mlp(esm_z)\n",
    "        else:\n",
    "            s_z_0 = s_s_0.new_zeros(B, L, L, self.cfg.trunk.pairwise_state_dim)\n",
    "\n",
    "        s_s_0 += self.embedding(aa)\n",
    "\n",
    "        structure: dict = self.trunk(\n",
    "            s_s_0, s_z_0, aa, residx, mask, no_recycles=num_recycles\n",
    "        )\n",
    "        # Documenting what we expect:\n",
    "        structure = {\n",
    "            k: v\n",
    "            for k, v in structure.items()\n",
    "            if k\n",
    "            in [\n",
    "                \"s_z\",\n",
    "                \"s_s\",\n",
    "                \"frames\",\n",
    "                \"sidechain_frames\",\n",
    "                \"unnormalized_angles\",\n",
    "                \"angles\",\n",
    "                \"positions\",\n",
    "                \"states\",\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        disto_logits = self.distogram_head(structure[\"s_z\"])\n",
    "        disto_logits = (disto_logits + disto_logits.transpose(1, 2)) / 2\n",
    "        structure[\"distogram_logits\"] = disto_logits\n",
    "\n",
    "        lm_logits = self.lm_head(structure[\"s_s\"])\n",
    "        structure[\"lm_logits\"] = lm_logits\n",
    "\n",
    "        structure[\"aatype\"] = aa\n",
    "        make_atom14_masks(structure)\n",
    "\n",
    "        for k in [\n",
    "            \"atom14_atom_exists\",\n",
    "            \"atom37_atom_exists\",\n",
    "        ]:\n",
    "            structure[k] *= mask.unsqueeze(-1)\n",
    "        structure[\"residue_index\"] = residx\n",
    "\n",
    "        lddt_head = self.lddt_head(structure[\"states\"]).reshape(\n",
    "            structure[\"states\"].shape[0], B, L, -1, self.lddt_bins\n",
    "        )\n",
    "        structure[\"lddt_head\"] = lddt_head\n",
    "        plddt = categorical_lddt(lddt_head[-1], bins=self.lddt_bins)\n",
    "        structure[\"plddt\"] = (\n",
    "            100 * plddt\n",
    "        )  # we predict plDDT between 0 and 1, scale to be between 0 and 100.\n",
    "\n",
    "        ptm_logits = self.ptm_head(structure[\"s_z\"])\n",
    "\n",
    "        seqlen = mask.type(torch.int64).sum(1)\n",
    "        structure[\"ptm_logits\"] = ptm_logits\n",
    "        structure[\"ptm\"] = torch.stack(\n",
    "            [\n",
    "                compute_tm(\n",
    "                    batch_ptm_logits[None, :sl, :sl],\n",
    "                    max_bins=31,\n",
    "                    no_bins=self.distogram_bins,\n",
    "                )\n",
    "                for batch_ptm_logits, sl in zip(ptm_logits, seqlen)\n",
    "            ]\n",
    "        )\n",
    "        structure.update(\n",
    "            compute_predicted_aligned_error(\n",
    "                ptm_logits, max_bin=31, no_bins=self.distogram_bins\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return structure\n",
    "    \n",
    "    def set_chunk_size(self, chunk_size: T.Optional[int]):\n",
    "        # This parameter means the axial attention will be computed\n",
    "        # in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).\n",
    "        # It's equivalent to running a for loop over chunks of the dimension we're iterative over,\n",
    "        # where the chunk_size is the size of the chunks, so 128 would mean to parse 128-lengthed chunks.\n",
    "        # Setting the value to None will return to default behavior, disable chunking.\n",
    "        self.trunk.set_chunk_size(chunk_size)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.esm_s_combine.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08792dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main model loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"esm2_t36_3B_UR50D\"\n",
    "\n",
    "# Load the main model data\n",
    "model_data = torch.load(f'../model/{MODEL_NAME}.pt', \n",
    "                        mmap=True, weights_only=False)\n",
    "print(\"Main model loaded\")\n",
    "\n",
    "# Get model, alphabet, and upgraded state_dict\n",
    "esm, alphabet, esm_dict = _load_model_and_alphabet_core_v2(model_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41535ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys:  {'esm.layers.21.fc2.weight', 'esm.layers.31.self_attn.q_proj.bias', 'esm.layers.2.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn_layer_norm.weight', 'esm.layers.9.final_layer_norm.weight', 'esm.layers.24.self_attn.v_proj.weight', 'esm.layers.34.self_attn_layer_norm.bias', 'esm.layers.26.self_attn.k_proj.bias', 'esm.layers.28.self_attn.v_proj.weight', 'esm.layers.8.self_attn_layer_norm.bias', 'esm.layers.10.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn_layer_norm.bias', 'esm.layers.34.self_attn.q_proj.weight', 'esm.layers.6.final_layer_norm.bias', 'esm.layers.31.fc1.bias', 'esm.layers.0.self_attn.out_proj.weight', 'esm.layers.20.self_attn.k_proj.weight', 'esm.layers.10.self_attn_layer_norm.bias', 'esm.layers.7.self_attn.q_proj.bias', 'esm.layers.3.self_attn.q_proj.weight', 'esm.layers.15.self_attn.v_proj.bias', 'esm.layers.22.self_attn.v_proj.weight', 'esm.layers.25.self_attn.rot_emb.inv_freq', 'esm.layers.0.fc2.weight', 'esm.layers.25.fc1.weight', 'esm.layers.25.final_layer_norm.bias', 'esm.layers.1.self_attn.k_proj.bias', 'esm.layers.21.self_attn.out_proj.weight', 'esm.layers.11.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn.v_proj.weight', 'esm.layers.28.fc2.bias', 'esm.layers.23.self_attn.k_proj.weight', 'esm.layers.13.final_layer_norm.weight', 'esm.layers.30.self_attn_layer_norm.bias', 'esm.layers.25.self_attn.k_proj.bias', 'esm.layers.11.fc1.weight', 'esm.layers.35.final_layer_norm.weight', 'esm.layers.1.self_attn.out_proj.weight', 'esm.layers.16.fc1.bias', 'esm.layers.32.self_attn.k_proj.weight', 'esm.layers.27.fc2.weight', 'esm.layers.20.final_layer_norm.bias', 'esm.layers.22.self_attn.rot_emb.inv_freq', 'esm.layers.1.fc1.bias', 'esm.layers.27.self_attn_layer_norm.bias', 'esm.layers.3.final_layer_norm.bias', 'esm.layers.5.final_layer_norm.weight', 'esm.layers.25.fc2.weight', 'esm.layers.16.self_attn.q_proj.bias', 'esm.layers.30.self_attn.q_proj.bias', 'esm.layers.13.final_layer_norm.bias', 'esm.layers.2.self_attn_layer_norm.bias', 'esm.layers.31.self_attn.rot_emb.inv_freq', 'esm.layers.16.self_attn.k_proj.weight', 'esm.layers.35.self_attn.q_proj.bias', 'esm.layers.23.fc1.bias', 'esm.layers.3.self_attn.v_proj.weight', 'esm.layers.15.self_attn.k_proj.bias', 'esm.layers.0.fc1.bias', 'esm.layers.26.fc2.weight', 'esm.layers.11.self_attn.out_proj.bias', 'esm.layers.19.final_layer_norm.weight', 'esm.layers.28.self_attn_layer_norm.weight', 'esm.layers.4.self_attn.rot_emb.inv_freq', 'esm.layers.7.fc1.bias', 'esm.layers.4.self_attn.k_proj.weight', 'esm.layers.25.self_attn_layer_norm.bias', 'esm.layers.14.fc1.weight', 'esm.layers.10.final_layer_norm.weight', 'esm.layers.7.self_attn_layer_norm.bias', 'esm.layers.27.self_attn.k_proj.weight', 'esm.layers.19.self_attn.k_proj.bias', 'esm.layers.23.self_attn_layer_norm.bias', 'esm.layers.29.self_attn.k_proj.weight', 'esm.layers.2.self_attn.v_proj.bias', 'esm.layers.9.self_attn.v_proj.weight', 'esm.layers.4.self_attn.q_proj.weight', 'esm.layers.14.self_attn.k_proj.bias', 'esm.layers.15.final_layer_norm.bias', 'esm.layers.20.self_attn.v_proj.weight', 'esm.layers.26.self_attn.q_proj.bias', 'esm.layers.20.self_attn.rot_emb.inv_freq', 'esm.layers.2.final_layer_norm.bias', 'esm.layers.29.final_layer_norm.bias', 'esm.layers.26.fc2.bias', 'esm.layers.12.self_attn.k_proj.weight', 'esm.layers.29.self_attn.out_proj.bias', 'esm.layers.8.self_attn.q_proj.weight', 'esm.layers.14.final_layer_norm.bias', 'esm.layers.18.fc2.bias', 'esm.layers.28.fc1.bias', 'esm.layers.2.self_attn.v_proj.weight', 'esm.layers.8.self_attn.v_proj.weight', 'esm.layers.16.self_attn.v_proj.weight', 'esm.layers.23.self_attn.v_proj.weight', 'esm.layers.32.self_attn.k_proj.bias', 'esm.layers.8.fc1.weight', 'esm.layers.11.self_attn.q_proj.bias', 'esm.layers.22.self_attn.out_proj.bias', 'esm.layers.29.fc2.bias', 'esm.layers.13.fc2.bias', 'esm.layers.33.self_attn.k_proj.bias', 'esm.layers.10.final_layer_norm.bias', 'esm.layers.19.final_layer_norm.bias', 'esm.layers.17.self_attn_layer_norm.bias', 'esm.layers.25.self_attn.q_proj.weight', 'esm.layers.1.self_attn.v_proj.weight', 'esm.layers.17.self_attn.rot_emb.inv_freq', 'esm.layers.1.self_attn.v_proj.bias', 'esm.layers.12.self_attn.q_proj.weight', 'esm.layers.34.fc2.weight', 'esm.layers.28.fc2.weight', 'esm.layers.27.self_attn_layer_norm.weight', 'esm.layers.30.fc1.bias', 'esm.layers.8.self_attn.v_proj.bias', 'esm.layers.16.self_attn_layer_norm.weight', 'esm.layers.20.fc1.weight', 'esm.layers.28.self_attn.out_proj.weight', 'esm.layers.31.self_attn.v_proj.weight', 'esm.layers.28.final_layer_norm.weight', 'esm.layers.11.self_attn.v_proj.bias', 'esm.layers.6.self_attn.v_proj.bias', 'esm.layers.15.fc2.bias', 'esm.layers.1.final_layer_norm.bias', 'esm.layers.30.self_attn.v_proj.weight', 'esm.layers.2.self_attn_layer_norm.weight', 'esm.layers.12.fc2.bias', 'esm.layers.10.fc1.bias', 'esm.layers.30.self_attn.out_proj.weight', 'esm.layers.12.fc1.weight', 'esm.layers.12.self_attn_layer_norm.weight', 'esm.layers.18.self_attn.k_proj.weight', 'esm.layers.16.self_attn.v_proj.bias', 'esm.layers.16.final_layer_norm.bias', 'esm.layers.11.self_attn_layer_norm.weight', 'esm.layers.32.self_attn_layer_norm.bias', 'esm.layers.17.final_layer_norm.weight', 'esm.layers.27.fc1.weight', 'esm.layers.32.fc2.weight', 'esm.layers.15.fc1.bias', 'esm.layers.8.fc2.bias', 'esm.layers.28.self_attn.q_proj.bias', 'esm.layers.25.fc1.bias', 'esm.layers.0.self_attn.rot_emb.inv_freq', 'esm.layers.18.fc2.weight', 'esm.layers.4.self_attn_layer_norm.weight', 'esm.layers.7.self_attn.out_proj.weight', 'esm.layers.33.self_attn.q_proj.bias', 'esm.layers.3.fc1.weight', 'esm.layers.30.fc2.weight', 'esm.layers.28.self_attn.k_proj.bias', 'esm.layers.5.self_attn.q_proj.weight', 'esm.layers.21.self_attn_layer_norm.bias', 'esm.layers.31.self_attn.out_proj.bias', 'esm.layers.20.self_attn.out_proj.weight', 'esm.layers.32.fc1.bias', 'esm.layers.35.self_attn_layer_norm.weight', 'esm.layers.6.fc2.bias', 'esm.layers.22.fc1.weight', 'esm.layers.26.self_attn.v_proj.weight', 'esm.layers.9.self_attn.k_proj.weight', 'esm.layers.35.self_attn.v_proj.weight', 'esm.layers.17.fc1.bias', 'esm.layers.14.self_attn.v_proj.bias', 'esm.layers.26.self_attn.v_proj.bias', 'esm.layers.17.fc2.weight', 'esm.layers.24.fc1.weight', 'esm.layers.8.self_attn.k_proj.weight', 'esm.layers.16.self_attn_layer_norm.bias', 'esm.layers.24.final_layer_norm.weight', 'esm.layers.13.self_attn.v_proj.bias', 'esm.layers.28.self_attn.out_proj.bias', 'esm.layers.11.self_attn.k_proj.bias', 'esm.layers.8.self_attn.q_proj.bias', 'esm.layers.3.fc1.bias', 'esm.layers.13.fc1.weight', 'esm.layers.5.fc2.bias', 'esm.layers.3.self_attn.rot_emb.inv_freq', 'esm.layers.0.self_attn_layer_norm.bias', 'esm.layers.5.fc1.weight', 'esm.layers.33.fc1.weight', 'esm.layers.2.self_attn.out_proj.weight', 'esm.layers.17.self_attn_layer_norm.weight', 'esm.layers.3.self_attn.k_proj.weight', 'esm.layers.6.self_attn.k_proj.bias', 'esm.layers.26.fc1.weight', 'esm.layers.6.self_attn.q_proj.weight', 'esm.layers.2.self_attn.out_proj.bias', 'esm.layers.9.self_attn.k_proj.bias', 'esm.layers.7.fc1.weight', 'esm.layers.28.self_attn.k_proj.weight', 'esm.layers.30.self_attn.rot_emb.inv_freq', 'esm.layers.28.final_layer_norm.bias', 'esm.layers.22.self_attn.q_proj.weight', 'esm.layers.8.self_attn.out_proj.weight', 'esm.layers.28.fc1.weight', 'esm.layers.29.self_attn.q_proj.bias', 'esm.layers.12.self_attn_layer_norm.bias', 'esm.layers.13.self_attn.rot_emb.inv_freq', 'esm.layers.10.self_attn.v_proj.weight', 'esm.layers.7.self_attn.out_proj.bias', 'esm.layers.2.fc2.bias', 'esm.layers.6.self_attn.out_proj.weight', 'esm.layers.34.fc2.bias', 'esm.layers.13.self_attn.out_proj.bias', 'esm.layers.13.fc1.bias', 'esm.layers.27.self_attn.v_proj.bias', 'esm.layers.0.fc1.weight', 'esm.layers.14.self_attn.q_proj.bias', 'esm.layers.20.self_attn.q_proj.bias', 'esm.layers.12.final_layer_norm.bias', 'esm.layers.15.self_attn_layer_norm.weight', 'esm.layers.18.fc1.bias', 'esm.layers.6.self_attn.out_proj.bias', 'esm.layers.4.fc2.bias', 'esm.layers.23.self_attn.out_proj.bias', 'esm.layers.1.self_attn.q_proj.weight', 'esm.layers.11.fc2.bias', 'esm.layers.0.self_attn.q_proj.bias', 'esm.layers.9.fc2.bias', 'esm.layers.10.self_attn.k_proj.weight', 'esm.layers.22.fc2.weight', 'esm.layers.7.self_attn.v_proj.weight', 'esm.layers.1.fc1.weight', 'esm.layers.31.self_attn.q_proj.weight', 'esm.layers.12.fc1.bias', 'esm.layers.23.fc1.weight', 'esm.layers.13.self_attn.out_proj.weight', 'esm.layers.3.fc2.weight', 'esm.layers.35.fc2.bias', 'esm.layers.18.self_attn.out_proj.bias', 'esm.layers.14.self_attn_layer_norm.weight', 'esm.layers.15.self_attn_layer_norm.bias', 'esm.layers.29.self_attn.v_proj.weight', 'esm.layers.15.self_attn.q_proj.weight', 'esm.layers.20.fc2.weight', 'esm.layers.34.self_attn.q_proj.bias', 'esm.layers.32.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn.out_proj.bias', 'esm.layers.16.fc1.weight', 'esm.layers.25.self_attn_layer_norm.weight', 'esm.layers.29.fc1.bias', 'esm.layers.21.self_attn.k_proj.bias', 'esm.layers.3.self_attn.out_proj.weight', 'esm.layers.4.final_layer_norm.bias', 'esm.layers.20.final_layer_norm.weight', 'esm.layers.29.self_attn_layer_norm.weight', 'esm.layers.11.self_attn.out_proj.weight', 'esm.layers.6.final_layer_norm.weight', 'esm.layers.13.self_attn_layer_norm.weight', 'esm.layers.33.self_attn.out_proj.bias', 'esm.layers.20.self_attn_layer_norm.bias', 'esm.layers.10.self_attn.out_proj.bias', 'esm.layers.18.self_attn_layer_norm.weight', 'esm.layers.9.self_attn.out_proj.bias', 'esm.layers.15.fc2.weight', 'esm.layers.18.self_attn.q_proj.bias', 'esm.layers.33.self_attn.q_proj.weight', 'esm.layers.30.final_layer_norm.weight', 'esm.contact_head.regression.weight', 'esm.layers.27.self_attn.rot_emb.inv_freq', 'esm.layers.2.fc1.weight', 'esm.layers.15.self_attn.k_proj.weight', 'esm.layers.4.self_attn.k_proj.bias', 'esm.layers.5.self_attn.v_proj.bias', 'esm.layers.29.self_attn.rot_emb.inv_freq', 'esm.layers.29.fc2.weight', 'esm.layers.6.self_attn.q_proj.bias', 'esm.layers.15.fc1.weight', 'esm.layers.4.self_attn.out_proj.weight', 'esm.layers.10.self_attn.out_proj.weight', 'esm.layers.35.fc2.weight', 'esm.layers.0.self_attn.k_proj.weight', 'esm.layers.14.self_attn_layer_norm.bias', 'esm.layers.3.self_attn.k_proj.bias', 'esm.layers.0.self_attn.v_proj.bias', 'esm.layers.31.self_attn_layer_norm.weight', 'esm.layers.21.self_attn.q_proj.bias', 'esm.layers.30.fc1.weight', 'esm.layers.3.self_attn.out_proj.bias', 'esm.layers.8.fc2.weight', 'esm.layers.10.fc2.weight', 'esm.layers.33.final_layer_norm.bias', 'esm.layers.16.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn.v_proj.bias', 'esm.layers.1.self_attn.rot_emb.inv_freq', 'esm.layers.22.fc2.bias', 'esm.layers.8.self_attn.out_proj.bias', 'esm.layers.5.self_attn.out_proj.weight', 'esm.layers.8.self_attn.k_proj.bias', 'esm.layers.18.self_attn.out_proj.weight', 'esm.layers.35.self_attn.out_proj.bias', 'esm.layers.12.self_attn.out_proj.bias', 'esm.layers.9.fc1.bias', 'esm.layers.5.fc1.bias', 'esm.layers.4.fc1.bias', 'esm.layers.21.fc1.weight', 'esm.layers.0.fc2.bias', 'esm.layers.35.self_attn.k_proj.weight', 'esm.layers.9.self_attn.out_proj.weight', 'esm.layers.32.self_attn_layer_norm.weight', 'esm.layers.9.self_attn.v_proj.bias', 'esm.layers.17.self_attn.q_proj.weight', 'esm.layers.5.self_attn.out_proj.bias', 'esm.layers.3.fc2.bias', 'esm.lm_head.dense.bias', 'esm.layers.30.self_attn_layer_norm.weight', 'esm.layers.2.fc1.bias', 'esm.layers.3.self_attn.q_proj.bias', 'esm.layers.17.self_attn.v_proj.bias', 'esm.layers.28.self_attn.rot_emb.inv_freq', 'esm.layers.33.self_attn.k_proj.weight', 'esm.layers.8.self_attn_layer_norm.weight', 'esm.layers.13.self_attn.q_proj.weight', 'esm.layers.0.self_attn.out_proj.bias', 'esm.layers.30.fc2.bias', 'esm.layers.10.fc2.bias', 'esm.layers.1.self_attn.k_proj.weight', 'esm.layers.29.final_layer_norm.weight', 'esm.layers.9.self_attn.q_proj.bias', 'esm.layers.7.self_attn.rot_emb.inv_freq', 'esm.layers.17.self_attn.k_proj.bias', 'esm.layers.25.self_attn.out_proj.weight', 'esm.layers.27.self_attn.v_proj.weight', 'esm.layers.27.fc2.bias', 'esm.layers.35.self_attn.k_proj.bias', 'esm.layers.35.self_attn.rot_emb.inv_freq', 'esm.layers.14.fc1.bias', 'esm.layers.24.self_attn.q_proj.weight', 'esm.layers.6.fc2.weight', 'esm.lm_head.dense.weight', 'esm.layers.35.self_attn_layer_norm.bias', 'esm.layers.16.self_attn.k_proj.bias', 'esm.layers.15.self_attn.out_proj.bias', 'esm.layers.31.self_attn.out_proj.weight', 'esm.layers.30.self_attn.k_proj.bias', 'esm.layers.7.final_layer_norm.bias', 'esm.layers.1.self_attn_layer_norm.bias', 'esm.layers.4.fc2.weight', 'esm.layers.25.self_attn.v_proj.weight', 'esm.layers.33.fc2.weight', 'esm.layers.31.final_layer_norm.weight', 'esm.layers.19.fc1.bias', 'esm.layers.20.self_attn.out_proj.bias', 'esm.layers.30.self_attn.v_proj.bias', 'esm.layers.13.fc2.weight', 'esm.layers.23.fc2.bias', 'esm.layers.24.self_attn_layer_norm.weight', 'esm.layers.33.self_attn.rot_emb.inv_freq', 'esm.layers.16.self_attn.q_proj.weight', 'esm.layers.0.final_layer_norm.weight', 'esm.layers.27.self_attn.q_proj.bias', 'esm.layers.32.final_layer_norm.bias', 'esm.layers.23.final_layer_norm.weight', 'esm.layers.32.fc2.bias', 'esm.layers.22.self_attn_layer_norm.bias', 'esm.layers.31.fc1.weight', 'esm.layers.21.final_layer_norm.weight', 'esm.layers.18.fc1.weight', 'esm.layers.10.self_attn.q_proj.bias', 'esm.layers.22.self_attn.k_proj.bias', 'esm.layers.1.self_attn.q_proj.bias', 'esm.layers.20.self_attn.k_proj.bias', 'esm.layers.26.self_attn.rot_emb.inv_freq', 'esm.layers.33.self_attn.v_proj.weight', 'esm.layers.9.self_attn.q_proj.weight', 'esm.layers.27.self_attn.out_proj.bias', 'esm.layers.18.self_attn_layer_norm.bias', 'esm.layers.29.self_attn_layer_norm.bias', 'esm.layers.7.fc2.weight', 'esm.layers.13.self_attn.k_proj.bias', 'esm.layers.19.self_attn.q_proj.weight', 'esm.layers.0.final_layer_norm.bias', 'esm.layers.15.self_attn.out_proj.weight', 'esm.layers.33.fc1.bias', 'esm.layers.17.self_attn.out_proj.bias', 'esm.layers.0.self_attn.k_proj.bias', 'esm.layers.28.self_attn.q_proj.weight', 'esm.layers.17.final_layer_norm.bias', 'esm.layers.11.self_attn_layer_norm.bias', 'esm.layers.26.fc1.bias', 'esm.layers.25.self_attn.q_proj.bias', 'esm.layers.28.self_attn_layer_norm.bias', 'esm.layers.6.fc1.weight', 'esm.layers.18.self_attn.k_proj.bias', 'esm.layers.24.self_attn.v_proj.bias', 'esm.embed_tokens.weight', 'esm.layers.20.fc1.bias', 'esm.layers.32.self_attn.out_proj.weight', 'esm.layers.29.self_attn.out_proj.weight', 'esm.layers.5.self_attn.v_proj.weight', 'esm.layers.16.fc2.weight', 'esm.emb_layer_norm_after.weight', 'esm.layers.25.self_attn.v_proj.bias', 'esm.layers.21.self_attn.v_proj.bias', 'esm.layers.35.self_attn.out_proj.weight', 'esm.layers.11.self_attn.k_proj.weight', 'esm.layers.35.fc1.weight', 'esm.layers.19.self_attn.k_proj.weight', 'esm.layers.2.self_attn.k_proj.weight', 'esm.layers.33.self_attn.out_proj.weight', 'esm.layers.34.final_layer_norm.bias', 'esm.layers.9.fc2.weight', 'esm.layers.11.final_layer_norm.weight', 'esm.layers.16.final_layer_norm.weight', 'esm.layers.0.self_attn.q_proj.weight', 'esm.layers.23.self_attn.out_proj.weight', 'esm.layers.22.self_attn.q_proj.bias', 'esm.layers.21.self_attn.rot_emb.inv_freq', 'esm.layers.23.fc2.weight', 'esm.layers.3.self_attn_layer_norm.bias', 'esm.layers.35.self_attn.q_proj.weight', 'esm.layers.5.self_attn.k_proj.weight', 'esm.layers.24.self_attn.q_proj.bias', 'esm.layers.16.self_attn.out_proj.weight', 'esm.layers.24.self_attn.k_proj.weight', 'esm.layers.30.final_layer_norm.bias', 'esm.layers.5.self_attn_layer_norm.weight', 'esm.layers.11.fc2.weight', 'esm.layers.5.self_attn_layer_norm.bias', 'esm.layers.25.final_layer_norm.weight', 'esm.layers.12.self_attn.q_proj.bias', 'esm.layers.2.final_layer_norm.weight', 'esm.layers.11.self_attn.v_proj.weight', 'esm.layers.20.self_attn.v_proj.bias', 'esm.layers.11.final_layer_norm.bias', 'esm.layers.8.self_attn.rot_emb.inv_freq', 'esm.layers.32.self_attn.v_proj.weight', 'esm.layers.17.self_attn.out_proj.weight', 'esm.layers.4.final_layer_norm.weight', 'esm.layers.2.self_attn.q_proj.bias', 'esm.lm_head.layer_norm.weight', 'esm.layers.35.final_layer_norm.bias', 'esm.layers.24.fc1.bias', 'esm.layers.4.self_attn.out_proj.bias', 'esm.layers.16.self_attn.out_proj.bias', 'esm.layers.6.self_attn.rot_emb.inv_freq', 'esm.layers.24.final_layer_norm.bias', 'esm.layers.19.fc2.bias', 'esm.layers.34.self_attn.v_proj.bias', 'esm.layers.13.self_attn.v_proj.weight', 'esm.layers.19.self_attn_layer_norm.weight', 'esm.layers.22.self_attn.v_proj.bias', 'esm.layers.23.self_attn.rot_emb.inv_freq', 'esm.layers.16.fc2.bias', 'esm.layers.14.fc2.weight', 'esm.layers.7.final_layer_norm.weight', 'esm.layers.27.final_layer_norm.bias', 'esm.layers.26.self_attn.out_proj.weight', 'esm.layers.21.fc2.bias', 'esm.layers.31.self_attn.k_proj.weight', 'esm.layers.26.self_attn_layer_norm.bias', 'esm.layers.31.fc2.bias', 'esm.layers.1.self_attn_layer_norm.weight', 'esm.layers.5.self_attn.k_proj.bias', 'esm.layers.2.self_attn.k_proj.bias', 'esm.layers.6.self_attn.k_proj.weight', 'esm.contact_head.regression.bias', 'esm.layers.19.fc1.weight', 'esm.layers.24.fc2.weight', 'esm.layers.19.self_attn.q_proj.bias', 'esm.layers.19.self_attn.v_proj.bias', 'esm.layers.28.self_attn.v_proj.bias', 'esm.layers.33.fc2.bias', 'esm.layers.23.final_layer_norm.bias', 'esm.layers.4.self_attn_layer_norm.bias', 'esm.layers.30.self_attn.out_proj.bias', 'esm.layers.34.self_attn_layer_norm.weight', 'esm.layers.20.fc2.bias', 'esm.layers.18.final_layer_norm.weight', 'esm.layers.9.final_layer_norm.bias', 'esm.layers.19.self_attn_layer_norm.bias', 'esm.layers.25.fc2.bias', 'esm.layers.21.self_attn.q_proj.weight', 'esm.layers.20.self_attn.q_proj.weight', 'esm.layers.0.self_attn_layer_norm.weight', 'esm.layers.5.self_attn.rot_emb.inv_freq', 'esm.layers.27.self_attn.q_proj.weight', 'esm.layers.31.self_attn.v_proj.bias', 'esm.layers.14.self_attn.out_proj.bias', 'esm.layers.21.final_layer_norm.bias', 'esm.layers.14.self_attn.k_proj.weight', 'esm.layers.17.self_attn.v_proj.weight', 'esm.layers.18.self_attn.v_proj.weight', 'esm.layers.18.self_attn.q_proj.weight', 'esm.layers.33.final_layer_norm.weight', 'esm.layers.11.self_attn.q_proj.weight', 'esm.layers.2.fc2.weight', 'esm.layers.26.self_attn_layer_norm.weight', 'esm.layers.29.self_attn.q_proj.weight', 'esm.layers.10.self_attn.k_proj.bias', 'esm.layers.19.self_attn.out_proj.weight', 'esm.emb_layer_norm_after.bias', 'esm.layers.32.self_attn.q_proj.weight', 'esm.layers.5.fc2.weight', 'esm.layers.12.self_attn.v_proj.weight', 'esm.layers.3.final_layer_norm.weight', 'esm.layers.18.self_attn.rot_emb.inv_freq', 'esm.layers.32.self_attn.q_proj.bias', 'esm.layers.4.fc1.weight', 'esm.layers.19.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn.v_proj.bias', 'esm.layers.14.fc2.bias', 'esm.layers.34.self_attn.out_proj.bias', 'esm.layers.5.self_attn.q_proj.bias', 'esm.layers.14.self_attn.rot_emb.inv_freq', 'esm.layers.17.self_attn.k_proj.weight', 'esm.layers.33.self_attn_layer_norm.bias', 'esm.layers.19.self_attn.v_proj.weight', 'esm.layers.14.self_attn.q_proj.weight', 'esm.layers.18.self_attn.v_proj.bias', 'esm.layers.1.fc2.weight', 'esm.layers.26.final_layer_norm.bias', 'esm.layers.17.fc1.weight', 'esm.layers.32.final_layer_norm.weight', 'esm.layers.0.self_attn.v_proj.weight', 'esm.layers.27.self_attn.out_proj.weight', 'esm.layers.31.self_attn.k_proj.bias', 'esm.layers.1.fc2.bias', 'esm.layers.15.final_layer_norm.weight', 'esm.layers.24.fc2.bias', 'esm.layers.10.self_attn_layer_norm.weight', 'esm.layers.21.self_attn_layer_norm.weight', 'esm.layers.25.self_attn.k_proj.weight', 'esm.layers.30.self_attn.k_proj.weight', 'esm.layers.7.fc2.bias', 'esm.layers.34.fc1.weight', 'esm.layers.34.final_layer_norm.weight', 'esm.layers.7.self_attn.v_proj.bias', 'esm.lm_head.bias', 'esm.layers.12.self_attn.out_proj.weight', 'esm.layers.34.self_attn.k_proj.bias', 'esm.layers.23.self_attn.v_proj.bias', 'esm.layers.20.self_attn_layer_norm.weight', 'esm.layers.8.final_layer_norm.weight', 'esm.layers.13.self_attn.q_proj.bias', 'esm.layers.23.self_attn.k_proj.bias', 'esm.layers.15.self_attn.q_proj.bias', 'esm.lm_head.weight', 'esm.layers.14.self_attn.v_proj.weight', 'esm.layers.34.self_attn.rot_emb.inv_freq', 'esm.layers.12.self_attn.rot_emb.inv_freq', 'esm.layers.34.self_attn.v_proj.weight', 'esm.layers.31.fc2.weight', 'esm.layers.7.self_attn_layer_norm.weight', 'esm.layers.33.self_attn_layer_norm.weight', 'esm.layers.26.final_layer_norm.weight', 'esm.layers.7.self_attn.k_proj.bias', 'esm.layers.2.self_attn.q_proj.weight', 'esm.layers.34.self_attn.k_proj.weight', 'esm.layers.27.final_layer_norm.weight', 'esm.layers.15.self_attn.rot_emb.inv_freq', 'esm.layers.19.self_attn.out_proj.bias', 'esm.layers.24.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn.k_proj.bias', 'esm.layers.15.self_attn.v_proj.weight', 'esm.layers.7.self_attn.k_proj.weight', 'esm.layers.35.self_attn.v_proj.bias', 'esm.layers.22.final_layer_norm.bias', 'esm.layers.27.self_attn.k_proj.bias', 'esm.layers.22.fc1.bias', 'esm.layers.25.self_attn.out_proj.bias', 'esm.layers.31.self_attn_layer_norm.bias', 'esm.layers.34.fc1.bias', 'esm.layers.27.fc1.bias', 'esm.layers.12.final_layer_norm.weight', 'esm.layers.10.self_attn.v_proj.bias', 'esm.layers.21.self_attn.v_proj.weight', 'esm.lm_head.layer_norm.bias', 'esm.layers.11.fc1.bias', 'esm.layers.13.self_attn_layer_norm.bias', 'esm.layers.13.self_attn.k_proj.weight', 'esm.layers.8.final_layer_norm.bias', 'esm.layers.6.self_attn.v_proj.weight', 'esm.layers.24.self_attn.out_proj.weight', 'esm.layers.12.self_attn.v_proj.bias', 'esm.layers.12.fc2.weight', 'esm.layers.4.self_attn.q_proj.bias', 'esm.layers.19.fc2.weight', 'esm.layers.22.self_attn.k_proj.weight', 'esm.layers.6.self_attn_layer_norm.weight', 'esm.layers.29.self_attn.v_proj.bias', 'esm.layers.32.fc1.weight', 'esm.layers.6.self_attn_layer_norm.bias', 'esm.layers.32.self_attn.out_proj.bias', 'esm.layers.7.self_attn.q_proj.weight', 'esm.layers.1.final_layer_norm.weight', 'esm.layers.34.self_attn.out_proj.weight', 'esm.layers.17.fc2.bias', 'esm.layers.9.self_attn_layer_norm.bias', 'esm.layers.31.final_layer_norm.bias', 'esm.layers.1.self_attn.out_proj.bias', 'esm.layers.23.self_attn.q_proj.weight', 'esm.layers.10.fc1.weight', 'esm.layers.12.self_attn.k_proj.bias', 'esm.layers.29.fc1.weight', 'esm.layers.17.self_attn.q_proj.bias', 'esm.layers.33.self_attn.v_proj.bias', 'esm.layers.9.self_attn_layer_norm.weight', 'esm.layers.10.self_attn.q_proj.weight', 'esm.layers.30.self_attn.q_proj.weight', 'esm.layers.9.fc1.weight', 'esm.layers.5.final_layer_norm.bias', 'esm.layers.21.self_attn.out_proj.bias', 'esm.layers.22.self_attn.out_proj.weight', 'esm.layers.26.self_attn.q_proj.weight', 'esm.layers.26.self_attn.k_proj.weight', 'esm.layers.22.final_layer_norm.weight', 'esm.layers.21.fc1.bias', 'esm.layers.14.final_layer_norm.weight', 'esm.layers.22.self_attn_layer_norm.weight', 'esm.layers.23.self_attn.q_proj.bias', 'esm.layers.26.self_attn.out_proj.bias', 'esm.layers.32.self_attn.v_proj.bias', 'esm.layers.23.self_attn_layer_norm.weight', 'esm.layers.14.self_attn.out_proj.weight', 'esm.layers.9.self_attn.rot_emb.inv_freq', 'esm.layers.6.fc1.bias', 'esm.layers.18.final_layer_norm.bias', 'esm.layers.21.self_attn.k_proj.weight', 'esm.layers.8.fc1.bias', 'esm.layers.35.fc1.bias', 'esm.layers.29.self_attn.k_proj.bias'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['esm.embed_tokens.weight', 'esm.layers.0.self_attn.k_proj.weight', 'esm.layers.0.self_attn.k_proj.bias', 'esm.layers.0.self_attn.v_proj.weight', 'esm.layers.0.self_attn.v_proj.bias', 'esm.layers.0.self_attn.q_proj.weight', 'esm.layers.0.self_attn.q_proj.bias', 'esm.layers.0.self_attn.out_proj.weight', 'esm.layers.0.self_attn.out_proj.bias', 'esm.layers.0.self_attn.rot_emb.inv_freq', 'esm.layers.0.self_attn_layer_norm.weight', 'esm.layers.0.self_attn_layer_norm.bias', 'esm.layers.0.fc1.weight', 'esm.layers.0.fc1.bias', 'esm.layers.0.fc2.weight', 'esm.layers.0.fc2.bias', 'esm.layers.0.final_layer_norm.weight', 'esm.layers.0.final_layer_norm.bias', 'esm.layers.1.self_attn.k_proj.weight', 'esm.layers.1.self_attn.k_proj.bias', 'esm.layers.1.self_attn.v_proj.weight', 'esm.layers.1.self_attn.v_proj.bias', 'esm.layers.1.self_attn.q_proj.weight', 'esm.layers.1.self_attn.q_proj.bias', 'esm.layers.1.self_attn.out_proj.weight', 'esm.layers.1.self_attn.out_proj.bias', 'esm.layers.1.self_attn.rot_emb.inv_freq', 'esm.layers.1.self_attn_layer_norm.weight', 'esm.layers.1.self_attn_layer_norm.bias', 'esm.layers.1.fc1.weight', 'esm.layers.1.fc1.bias', 'esm.layers.1.fc2.weight', 'esm.layers.1.fc2.bias', 'esm.layers.1.final_layer_norm.weight', 'esm.layers.1.final_layer_norm.bias', 'esm.layers.2.self_attn.k_proj.weight', 'esm.layers.2.self_attn.k_proj.bias', 'esm.layers.2.self_attn.v_proj.weight', 'esm.layers.2.self_attn.v_proj.bias', 'esm.layers.2.self_attn.q_proj.weight', 'esm.layers.2.self_attn.q_proj.bias', 'esm.layers.2.self_attn.out_proj.weight', 'esm.layers.2.self_attn.out_proj.bias', 'esm.layers.2.self_attn.rot_emb.inv_freq', 'esm.layers.2.self_attn_layer_norm.weight', 'esm.layers.2.self_attn_layer_norm.bias', 'esm.layers.2.fc1.weight', 'esm.layers.2.fc1.bias', 'esm.layers.2.fc2.weight', 'esm.layers.2.fc2.bias', 'esm.layers.2.final_layer_norm.weight', 'esm.layers.2.final_layer_norm.bias', 'esm.layers.3.self_attn.k_proj.weight', 'esm.layers.3.self_attn.k_proj.bias', 'esm.layers.3.self_attn.v_proj.weight', 'esm.layers.3.self_attn.v_proj.bias', 'esm.layers.3.self_attn.q_proj.weight', 'esm.layers.3.self_attn.q_proj.bias', 'esm.layers.3.self_attn.out_proj.weight', 'esm.layers.3.self_attn.out_proj.bias', 'esm.layers.3.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn_layer_norm.weight', 'esm.layers.3.self_attn_layer_norm.bias', 'esm.layers.3.fc1.weight', 'esm.layers.3.fc1.bias', 'esm.layers.3.fc2.weight', 'esm.layers.3.fc2.bias', 'esm.layers.3.final_layer_norm.weight', 'esm.layers.3.final_layer_norm.bias', 'esm.layers.4.self_attn.k_proj.weight', 'esm.layers.4.self_attn.k_proj.bias', 'esm.layers.4.self_attn.v_proj.weight', 'esm.layers.4.self_attn.v_proj.bias', 'esm.layers.4.self_attn.q_proj.weight', 'esm.layers.4.self_attn.q_proj.bias', 'esm.layers.4.self_attn.out_proj.weight', 'esm.layers.4.self_attn.out_proj.bias', 'esm.layers.4.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn_layer_norm.weight', 'esm.layers.4.self_attn_layer_norm.bias', 'esm.layers.4.fc1.weight', 'esm.layers.4.fc1.bias', 'esm.layers.4.fc2.weight', 'esm.layers.4.fc2.bias', 'esm.layers.4.final_layer_norm.weight', 'esm.layers.4.final_layer_norm.bias', 'esm.layers.5.self_attn.k_proj.weight', 'esm.layers.5.self_attn.k_proj.bias', 'esm.layers.5.self_attn.v_proj.weight', 'esm.layers.5.self_attn.v_proj.bias', 'esm.layers.5.self_attn.q_proj.weight', 'esm.layers.5.self_attn.q_proj.bias', 'esm.layers.5.self_attn.out_proj.weight', 'esm.layers.5.self_attn.out_proj.bias', 'esm.layers.5.self_attn.rot_emb.inv_freq', 'esm.layers.5.self_attn_layer_norm.weight', 'esm.layers.5.self_attn_layer_norm.bias', 'esm.layers.5.fc1.weight', 'esm.layers.5.fc1.bias', 'esm.layers.5.fc2.weight', 'esm.layers.5.fc2.bias', 'esm.layers.5.final_layer_norm.weight', 'esm.layers.5.final_layer_norm.bias', 'esm.layers.6.self_attn.k_proj.weight', 'esm.layers.6.self_attn.k_proj.bias', 'esm.layers.6.self_attn.v_proj.weight', 'esm.layers.6.self_attn.v_proj.bias', 'esm.layers.6.self_attn.q_proj.weight', 'esm.layers.6.self_attn.q_proj.bias', 'esm.layers.6.self_attn.out_proj.weight', 'esm.layers.6.self_attn.out_proj.bias', 'esm.layers.6.self_attn.rot_emb.inv_freq', 'esm.layers.6.self_attn_layer_norm.weight', 'esm.layers.6.self_attn_layer_norm.bias', 'esm.layers.6.fc1.weight', 'esm.layers.6.fc1.bias', 'esm.layers.6.fc2.weight', 'esm.layers.6.fc2.bias', 'esm.layers.6.final_layer_norm.weight', 'esm.layers.6.final_layer_norm.bias', 'esm.layers.7.self_attn.k_proj.weight', 'esm.layers.7.self_attn.k_proj.bias', 'esm.layers.7.self_attn.v_proj.weight', 'esm.layers.7.self_attn.v_proj.bias', 'esm.layers.7.self_attn.q_proj.weight', 'esm.layers.7.self_attn.q_proj.bias', 'esm.layers.7.self_attn.out_proj.weight', 'esm.layers.7.self_attn.out_proj.bias', 'esm.layers.7.self_attn.rot_emb.inv_freq', 'esm.layers.7.self_attn_layer_norm.weight', 'esm.layers.7.self_attn_layer_norm.bias', 'esm.layers.7.fc1.weight', 'esm.layers.7.fc1.bias', 'esm.layers.7.fc2.weight', 'esm.layers.7.fc2.bias', 'esm.layers.7.final_layer_norm.weight', 'esm.layers.7.final_layer_norm.bias', 'esm.layers.8.self_attn.k_proj.weight', 'esm.layers.8.self_attn.k_proj.bias', 'esm.layers.8.self_attn.v_proj.weight', 'esm.layers.8.self_attn.v_proj.bias', 'esm.layers.8.self_attn.q_proj.weight', 'esm.layers.8.self_attn.q_proj.bias', 'esm.layers.8.self_attn.out_proj.weight', 'esm.layers.8.self_attn.out_proj.bias', 'esm.layers.8.self_attn.rot_emb.inv_freq', 'esm.layers.8.self_attn_layer_norm.weight', 'esm.layers.8.self_attn_layer_norm.bias', 'esm.layers.8.fc1.weight', 'esm.layers.8.fc1.bias', 'esm.layers.8.fc2.weight', 'esm.layers.8.fc2.bias', 'esm.layers.8.final_layer_norm.weight', 'esm.layers.8.final_layer_norm.bias', 'esm.layers.9.self_attn.k_proj.weight', 'esm.layers.9.self_attn.k_proj.bias', 'esm.layers.9.self_attn.v_proj.weight', 'esm.layers.9.self_attn.v_proj.bias', 'esm.layers.9.self_attn.q_proj.weight', 'esm.layers.9.self_attn.q_proj.bias', 'esm.layers.9.self_attn.out_proj.weight', 'esm.layers.9.self_attn.out_proj.bias', 'esm.layers.9.self_attn.rot_emb.inv_freq', 'esm.layers.9.self_attn_layer_norm.weight', 'esm.layers.9.self_attn_layer_norm.bias', 'esm.layers.9.fc1.weight', 'esm.layers.9.fc1.bias', 'esm.layers.9.fc2.weight', 'esm.layers.9.fc2.bias', 'esm.layers.9.final_layer_norm.weight', 'esm.layers.9.final_layer_norm.bias', 'esm.layers.10.self_attn.k_proj.weight', 'esm.layers.10.self_attn.k_proj.bias', 'esm.layers.10.self_attn.v_proj.weight', 'esm.layers.10.self_attn.v_proj.bias', 'esm.layers.10.self_attn.q_proj.weight', 'esm.layers.10.self_attn.q_proj.bias', 'esm.layers.10.self_attn.out_proj.weight', 'esm.layers.10.self_attn.out_proj.bias', 'esm.layers.10.self_attn.rot_emb.inv_freq', 'esm.layers.10.self_attn_layer_norm.weight', 'esm.layers.10.self_attn_layer_norm.bias', 'esm.layers.10.fc1.weight', 'esm.layers.10.fc1.bias', 'esm.layers.10.fc2.weight', 'esm.layers.10.fc2.bias', 'esm.layers.10.final_layer_norm.weight', 'esm.layers.10.final_layer_norm.bias', 'esm.layers.11.self_attn.k_proj.weight', 'esm.layers.11.self_attn.k_proj.bias', 'esm.layers.11.self_attn.v_proj.weight', 'esm.layers.11.self_attn.v_proj.bias', 'esm.layers.11.self_attn.q_proj.weight', 'esm.layers.11.self_attn.q_proj.bias', 'esm.layers.11.self_attn.out_proj.weight', 'esm.layers.11.self_attn.out_proj.bias', 'esm.layers.11.self_attn.rot_emb.inv_freq', 'esm.layers.11.self_attn_layer_norm.weight', 'esm.layers.11.self_attn_layer_norm.bias', 'esm.layers.11.fc1.weight', 'esm.layers.11.fc1.bias', 'esm.layers.11.fc2.weight', 'esm.layers.11.fc2.bias', 'esm.layers.11.final_layer_norm.weight', 'esm.layers.11.final_layer_norm.bias', 'esm.layers.12.self_attn.k_proj.weight', 'esm.layers.12.self_attn.k_proj.bias', 'esm.layers.12.self_attn.v_proj.weight', 'esm.layers.12.self_attn.v_proj.bias', 'esm.layers.12.self_attn.q_proj.weight', 'esm.layers.12.self_attn.q_proj.bias', 'esm.layers.12.self_attn.out_proj.weight', 'esm.layers.12.self_attn.out_proj.bias', 'esm.layers.12.self_attn.rot_emb.inv_freq', 'esm.layers.12.self_attn_layer_norm.weight', 'esm.layers.12.self_attn_layer_norm.bias', 'esm.layers.12.fc1.weight', 'esm.layers.12.fc1.bias', 'esm.layers.12.fc2.weight', 'esm.layers.12.fc2.bias', 'esm.layers.12.final_layer_norm.weight', 'esm.layers.12.final_layer_norm.bias', 'esm.layers.13.self_attn.k_proj.weight', 'esm.layers.13.self_attn.k_proj.bias', 'esm.layers.13.self_attn.v_proj.weight', 'esm.layers.13.self_attn.v_proj.bias', 'esm.layers.13.self_attn.q_proj.weight', 'esm.layers.13.self_attn.q_proj.bias', 'esm.layers.13.self_attn.out_proj.weight', 'esm.layers.13.self_attn.out_proj.bias', 'esm.layers.13.self_attn.rot_emb.inv_freq', 'esm.layers.13.self_attn_layer_norm.weight', 'esm.layers.13.self_attn_layer_norm.bias', 'esm.layers.13.fc1.weight', 'esm.layers.13.fc1.bias', 'esm.layers.13.fc2.weight', 'esm.layers.13.fc2.bias', 'esm.layers.13.final_layer_norm.weight', 'esm.layers.13.final_layer_norm.bias', 'esm.layers.14.self_attn.k_proj.weight', 'esm.layers.14.self_attn.k_proj.bias', 'esm.layers.14.self_attn.v_proj.weight', 'esm.layers.14.self_attn.v_proj.bias', 'esm.layers.14.self_attn.q_proj.weight', 'esm.layers.14.self_attn.q_proj.bias', 'esm.layers.14.self_attn.out_proj.weight', 'esm.layers.14.self_attn.out_proj.bias', 'esm.layers.14.self_attn.rot_emb.inv_freq', 'esm.layers.14.self_attn_layer_norm.weight', 'esm.layers.14.self_attn_layer_norm.bias', 'esm.layers.14.fc1.weight', 'esm.layers.14.fc1.bias', 'esm.layers.14.fc2.weight', 'esm.layers.14.fc2.bias', 'esm.layers.14.final_layer_norm.weight', 'esm.layers.14.final_layer_norm.bias', 'esm.layers.15.self_attn.k_proj.weight', 'esm.layers.15.self_attn.k_proj.bias', 'esm.layers.15.self_attn.v_proj.weight', 'esm.layers.15.self_attn.v_proj.bias', 'esm.layers.15.self_attn.q_proj.weight', 'esm.layers.15.self_attn.q_proj.bias', 'esm.layers.15.self_attn.out_proj.weight', 'esm.layers.15.self_attn.out_proj.bias', 'esm.layers.15.self_attn.rot_emb.inv_freq', 'esm.layers.15.self_attn_layer_norm.weight', 'esm.layers.15.self_attn_layer_norm.bias', 'esm.layers.15.fc1.weight', 'esm.layers.15.fc1.bias', 'esm.layers.15.fc2.weight', 'esm.layers.15.fc2.bias', 'esm.layers.15.final_layer_norm.weight', 'esm.layers.15.final_layer_norm.bias', 'esm.layers.16.self_attn.k_proj.weight', 'esm.layers.16.self_attn.k_proj.bias', 'esm.layers.16.self_attn.v_proj.weight', 'esm.layers.16.self_attn.v_proj.bias', 'esm.layers.16.self_attn.q_proj.weight', 'esm.layers.16.self_attn.q_proj.bias', 'esm.layers.16.self_attn.out_proj.weight', 'esm.layers.16.self_attn.out_proj.bias', 'esm.layers.16.self_attn.rot_emb.inv_freq', 'esm.layers.16.self_attn_layer_norm.weight', 'esm.layers.16.self_attn_layer_norm.bias', 'esm.layers.16.fc1.weight', 'esm.layers.16.fc1.bias', 'esm.layers.16.fc2.weight', 'esm.layers.16.fc2.bias', 'esm.layers.16.final_layer_norm.weight', 'esm.layers.16.final_layer_norm.bias', 'esm.layers.17.self_attn.k_proj.weight', 'esm.layers.17.self_attn.k_proj.bias', 'esm.layers.17.self_attn.v_proj.weight', 'esm.layers.17.self_attn.v_proj.bias', 'esm.layers.17.self_attn.q_proj.weight', 'esm.layers.17.self_attn.q_proj.bias', 'esm.layers.17.self_attn.out_proj.weight', 'esm.layers.17.self_attn.out_proj.bias', 'esm.layers.17.self_attn.rot_emb.inv_freq', 'esm.layers.17.self_attn_layer_norm.weight', 'esm.layers.17.self_attn_layer_norm.bias', 'esm.layers.17.fc1.weight', 'esm.layers.17.fc1.bias', 'esm.layers.17.fc2.weight', 'esm.layers.17.fc2.bias', 'esm.layers.17.final_layer_norm.weight', 'esm.layers.17.final_layer_norm.bias', 'esm.layers.18.self_attn.k_proj.weight', 'esm.layers.18.self_attn.k_proj.bias', 'esm.layers.18.self_attn.v_proj.weight', 'esm.layers.18.self_attn.v_proj.bias', 'esm.layers.18.self_attn.q_proj.weight', 'esm.layers.18.self_attn.q_proj.bias', 'esm.layers.18.self_attn.out_proj.weight', 'esm.layers.18.self_attn.out_proj.bias', 'esm.layers.18.self_attn.rot_emb.inv_freq', 'esm.layers.18.self_attn_layer_norm.weight', 'esm.layers.18.self_attn_layer_norm.bias', 'esm.layers.18.fc1.weight', 'esm.layers.18.fc1.bias', 'esm.layers.18.fc2.weight', 'esm.layers.18.fc2.bias', 'esm.layers.18.final_layer_norm.weight', 'esm.layers.18.final_layer_norm.bias', 'esm.layers.19.self_attn.k_proj.weight', 'esm.layers.19.self_attn.k_proj.bias', 'esm.layers.19.self_attn.v_proj.weight', 'esm.layers.19.self_attn.v_proj.bias', 'esm.layers.19.self_attn.q_proj.weight', 'esm.layers.19.self_attn.q_proj.bias', 'esm.layers.19.self_attn.out_proj.weight', 'esm.layers.19.self_attn.out_proj.bias', 'esm.layers.19.self_attn.rot_emb.inv_freq', 'esm.layers.19.self_attn_layer_norm.weight', 'esm.layers.19.self_attn_layer_norm.bias', 'esm.layers.19.fc1.weight', 'esm.layers.19.fc1.bias', 'esm.layers.19.fc2.weight', 'esm.layers.19.fc2.bias', 'esm.layers.19.final_layer_norm.weight', 'esm.layers.19.final_layer_norm.bias', 'esm.layers.20.self_attn.k_proj.weight', 'esm.layers.20.self_attn.k_proj.bias', 'esm.layers.20.self_attn.v_proj.weight', 'esm.layers.20.self_attn.v_proj.bias', 'esm.layers.20.self_attn.q_proj.weight', 'esm.layers.20.self_attn.q_proj.bias', 'esm.layers.20.self_attn.out_proj.weight', 'esm.layers.20.self_attn.out_proj.bias', 'esm.layers.20.self_attn.rot_emb.inv_freq', 'esm.layers.20.self_attn_layer_norm.weight', 'esm.layers.20.self_attn_layer_norm.bias', 'esm.layers.20.fc1.weight', 'esm.layers.20.fc1.bias', 'esm.layers.20.fc2.weight', 'esm.layers.20.fc2.bias', 'esm.layers.20.final_layer_norm.weight', 'esm.layers.20.final_layer_norm.bias', 'esm.layers.21.self_attn.k_proj.weight', 'esm.layers.21.self_attn.k_proj.bias', 'esm.layers.21.self_attn.v_proj.weight', 'esm.layers.21.self_attn.v_proj.bias', 'esm.layers.21.self_attn.q_proj.weight', 'esm.layers.21.self_attn.q_proj.bias', 'esm.layers.21.self_attn.out_proj.weight', 'esm.layers.21.self_attn.out_proj.bias', 'esm.layers.21.self_attn.rot_emb.inv_freq', 'esm.layers.21.self_attn_layer_norm.weight', 'esm.layers.21.self_attn_layer_norm.bias', 'esm.layers.21.fc1.weight', 'esm.layers.21.fc1.bias', 'esm.layers.21.fc2.weight', 'esm.layers.21.fc2.bias', 'esm.layers.21.final_layer_norm.weight', 'esm.layers.21.final_layer_norm.bias', 'esm.layers.22.self_attn.k_proj.weight', 'esm.layers.22.self_attn.k_proj.bias', 'esm.layers.22.self_attn.v_proj.weight', 'esm.layers.22.self_attn.v_proj.bias', 'esm.layers.22.self_attn.q_proj.weight', 'esm.layers.22.self_attn.q_proj.bias', 'esm.layers.22.self_attn.out_proj.weight', 'esm.layers.22.self_attn.out_proj.bias', 'esm.layers.22.self_attn.rot_emb.inv_freq', 'esm.layers.22.self_attn_layer_norm.weight', 'esm.layers.22.self_attn_layer_norm.bias', 'esm.layers.22.fc1.weight', 'esm.layers.22.fc1.bias', 'esm.layers.22.fc2.weight', 'esm.layers.22.fc2.bias', 'esm.layers.22.final_layer_norm.weight', 'esm.layers.22.final_layer_norm.bias', 'esm.layers.23.self_attn.k_proj.weight', 'esm.layers.23.self_attn.k_proj.bias', 'esm.layers.23.self_attn.v_proj.weight', 'esm.layers.23.self_attn.v_proj.bias', 'esm.layers.23.self_attn.q_proj.weight', 'esm.layers.23.self_attn.q_proj.bias', 'esm.layers.23.self_attn.out_proj.weight', 'esm.layers.23.self_attn.out_proj.bias', 'esm.layers.23.self_attn.rot_emb.inv_freq', 'esm.layers.23.self_attn_layer_norm.weight', 'esm.layers.23.self_attn_layer_norm.bias', 'esm.layers.23.fc1.weight', 'esm.layers.23.fc1.bias', 'esm.layers.23.fc2.weight', 'esm.layers.23.fc2.bias', 'esm.layers.23.final_layer_norm.weight', 'esm.layers.23.final_layer_norm.bias', 'esm.layers.24.self_attn.k_proj.weight', 'esm.layers.24.self_attn.k_proj.bias', 'esm.layers.24.self_attn.v_proj.weight', 'esm.layers.24.self_attn.v_proj.bias', 'esm.layers.24.self_attn.q_proj.weight', 'esm.layers.24.self_attn.q_proj.bias', 'esm.layers.24.self_attn.out_proj.weight', 'esm.layers.24.self_attn.out_proj.bias', 'esm.layers.24.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn_layer_norm.weight', 'esm.layers.24.self_attn_layer_norm.bias', 'esm.layers.24.fc1.weight', 'esm.layers.24.fc1.bias', 'esm.layers.24.fc2.weight', 'esm.layers.24.fc2.bias', 'esm.layers.24.final_layer_norm.weight', 'esm.layers.24.final_layer_norm.bias', 'esm.layers.25.self_attn.k_proj.weight', 'esm.layers.25.self_attn.k_proj.bias', 'esm.layers.25.self_attn.v_proj.weight', 'esm.layers.25.self_attn.v_proj.bias', 'esm.layers.25.self_attn.q_proj.weight', 'esm.layers.25.self_attn.q_proj.bias', 'esm.layers.25.self_attn.out_proj.weight', 'esm.layers.25.self_attn.out_proj.bias', 'esm.layers.25.self_attn.rot_emb.inv_freq', 'esm.layers.25.self_attn_layer_norm.weight', 'esm.layers.25.self_attn_layer_norm.bias', 'esm.layers.25.fc1.weight', 'esm.layers.25.fc1.bias', 'esm.layers.25.fc2.weight', 'esm.layers.25.fc2.bias', 'esm.layers.25.final_layer_norm.weight', 'esm.layers.25.final_layer_norm.bias', 'esm.layers.26.self_attn.k_proj.weight', 'esm.layers.26.self_attn.k_proj.bias', 'esm.layers.26.self_attn.v_proj.weight', 'esm.layers.26.self_attn.v_proj.bias', 'esm.layers.26.self_attn.q_proj.weight', 'esm.layers.26.self_attn.q_proj.bias', 'esm.layers.26.self_attn.out_proj.weight', 'esm.layers.26.self_attn.out_proj.bias', 'esm.layers.26.self_attn.rot_emb.inv_freq', 'esm.layers.26.self_attn_layer_norm.weight', 'esm.layers.26.self_attn_layer_norm.bias', 'esm.layers.26.fc1.weight', 'esm.layers.26.fc1.bias', 'esm.layers.26.fc2.weight', 'esm.layers.26.fc2.bias', 'esm.layers.26.final_layer_norm.weight', 'esm.layers.26.final_layer_norm.bias', 'esm.layers.27.self_attn.k_proj.weight', 'esm.layers.27.self_attn.k_proj.bias', 'esm.layers.27.self_attn.v_proj.weight', 'esm.layers.27.self_attn.v_proj.bias', 'esm.layers.27.self_attn.q_proj.weight', 'esm.layers.27.self_attn.q_proj.bias', 'esm.layers.27.self_attn.out_proj.weight', 'esm.layers.27.self_attn.out_proj.bias', 'esm.layers.27.self_attn.rot_emb.inv_freq', 'esm.layers.27.self_attn_layer_norm.weight', 'esm.layers.27.self_attn_layer_norm.bias', 'esm.layers.27.fc1.weight', 'esm.layers.27.fc1.bias', 'esm.layers.27.fc2.weight', 'esm.layers.27.fc2.bias', 'esm.layers.27.final_layer_norm.weight', 'esm.layers.27.final_layer_norm.bias', 'esm.layers.28.self_attn.k_proj.weight', 'esm.layers.28.self_attn.k_proj.bias', 'esm.layers.28.self_attn.v_proj.weight', 'esm.layers.28.self_attn.v_proj.bias', 'esm.layers.28.self_attn.q_proj.weight', 'esm.layers.28.self_attn.q_proj.bias', 'esm.layers.28.self_attn.out_proj.weight', 'esm.layers.28.self_attn.out_proj.bias', 'esm.layers.28.self_attn.rot_emb.inv_freq', 'esm.layers.28.self_attn_layer_norm.weight', 'esm.layers.28.self_attn_layer_norm.bias', 'esm.layers.28.fc1.weight', 'esm.layers.28.fc1.bias', 'esm.layers.28.fc2.weight', 'esm.layers.28.fc2.bias', 'esm.layers.28.final_layer_norm.weight', 'esm.layers.28.final_layer_norm.bias', 'esm.layers.29.self_attn.k_proj.weight', 'esm.layers.29.self_attn.k_proj.bias', 'esm.layers.29.self_attn.v_proj.weight', 'esm.layers.29.self_attn.v_proj.bias', 'esm.layers.29.self_attn.q_proj.weight', 'esm.layers.29.self_attn.q_proj.bias', 'esm.layers.29.self_attn.out_proj.weight', 'esm.layers.29.self_attn.out_proj.bias', 'esm.layers.29.self_attn.rot_emb.inv_freq', 'esm.layers.29.self_attn_layer_norm.weight', 'esm.layers.29.self_attn_layer_norm.bias', 'esm.layers.29.fc1.weight', 'esm.layers.29.fc1.bias', 'esm.layers.29.fc2.weight', 'esm.layers.29.fc2.bias', 'esm.layers.29.final_layer_norm.weight', 'esm.layers.29.final_layer_norm.bias', 'esm.layers.30.self_attn.k_proj.weight', 'esm.layers.30.self_attn.k_proj.bias', 'esm.layers.30.self_attn.v_proj.weight', 'esm.layers.30.self_attn.v_proj.bias', 'esm.layers.30.self_attn.q_proj.weight', 'esm.layers.30.self_attn.q_proj.bias', 'esm.layers.30.self_attn.out_proj.weight', 'esm.layers.30.self_attn.out_proj.bias', 'esm.layers.30.self_attn.rot_emb.inv_freq', 'esm.layers.30.self_attn_layer_norm.weight', 'esm.layers.30.self_attn_layer_norm.bias', 'esm.layers.30.fc1.weight', 'esm.layers.30.fc1.bias', 'esm.layers.30.fc2.weight', 'esm.layers.30.fc2.bias', 'esm.layers.30.final_layer_norm.weight', 'esm.layers.30.final_layer_norm.bias', 'esm.layers.31.self_attn.k_proj.weight', 'esm.layers.31.self_attn.k_proj.bias', 'esm.layers.31.self_attn.v_proj.weight', 'esm.layers.31.self_attn.v_proj.bias', 'esm.layers.31.self_attn.q_proj.weight', 'esm.layers.31.self_attn.q_proj.bias', 'esm.layers.31.self_attn.out_proj.weight', 'esm.layers.31.self_attn.out_proj.bias', 'esm.layers.31.self_attn.rot_emb.inv_freq', 'esm.layers.31.self_attn_layer_norm.weight', 'esm.layers.31.self_attn_layer_norm.bias', 'esm.layers.31.fc1.weight', 'esm.layers.31.fc1.bias', 'esm.layers.31.fc2.weight', 'esm.layers.31.fc2.bias', 'esm.layers.31.final_layer_norm.weight', 'esm.layers.31.final_layer_norm.bias', 'esm.layers.32.self_attn.k_proj.weight', 'esm.layers.32.self_attn.k_proj.bias', 'esm.layers.32.self_attn.v_proj.weight', 'esm.layers.32.self_attn.v_proj.bias', 'esm.layers.32.self_attn.q_proj.weight', 'esm.layers.32.self_attn.q_proj.bias', 'esm.layers.32.self_attn.out_proj.weight', 'esm.layers.32.self_attn.out_proj.bias', 'esm.layers.32.self_attn.rot_emb.inv_freq', 'esm.layers.32.self_attn_layer_norm.weight', 'esm.layers.32.self_attn_layer_norm.bias', 'esm.layers.32.fc1.weight', 'esm.layers.32.fc1.bias', 'esm.layers.32.fc2.weight', 'esm.layers.32.fc2.bias', 'esm.layers.32.final_layer_norm.weight', 'esm.layers.32.final_layer_norm.bias', 'esm.layers.33.self_attn.k_proj.weight', 'esm.layers.33.self_attn.k_proj.bias', 'esm.layers.33.self_attn.v_proj.weight', 'esm.layers.33.self_attn.v_proj.bias', 'esm.layers.33.self_attn.q_proj.weight', 'esm.layers.33.self_attn.q_proj.bias', 'esm.layers.33.self_attn.out_proj.weight', 'esm.layers.33.self_attn.out_proj.bias', 'esm.layers.33.self_attn.rot_emb.inv_freq', 'esm.layers.33.self_attn_layer_norm.weight', 'esm.layers.33.self_attn_layer_norm.bias', 'esm.layers.33.fc1.weight', 'esm.layers.33.fc1.bias', 'esm.layers.33.fc2.weight', 'esm.layers.33.fc2.bias', 'esm.layers.33.final_layer_norm.weight', 'esm.layers.33.final_layer_norm.bias', 'esm.layers.34.self_attn.k_proj.weight', 'esm.layers.34.self_attn.k_proj.bias', 'esm.layers.34.self_attn.v_proj.weight', 'esm.layers.34.self_attn.v_proj.bias', 'esm.layers.34.self_attn.q_proj.weight', 'esm.layers.34.self_attn.q_proj.bias', 'esm.layers.34.self_attn.out_proj.weight', 'esm.layers.34.self_attn.out_proj.bias', 'esm.layers.34.self_attn.rot_emb.inv_freq', 'esm.layers.34.self_attn_layer_norm.weight', 'esm.layers.34.self_attn_layer_norm.bias', 'esm.layers.34.fc1.weight', 'esm.layers.34.fc1.bias', 'esm.layers.34.fc2.weight', 'esm.layers.34.fc2.bias', 'esm.layers.34.final_layer_norm.weight', 'esm.layers.34.final_layer_norm.bias', 'esm.layers.35.self_attn.k_proj.weight', 'esm.layers.35.self_attn.k_proj.bias', 'esm.layers.35.self_attn.v_proj.weight', 'esm.layers.35.self_attn.v_proj.bias', 'esm.layers.35.self_attn.q_proj.weight', 'esm.layers.35.self_attn.q_proj.bias', 'esm.layers.35.self_attn.out_proj.weight', 'esm.layers.35.self_attn.out_proj.bias', 'esm.layers.35.self_attn.rot_emb.inv_freq', 'esm.layers.35.self_attn_layer_norm.weight', 'esm.layers.35.self_attn_layer_norm.bias', 'esm.layers.35.fc1.weight', 'esm.layers.35.fc1.bias', 'esm.layers.35.fc2.weight', 'esm.layers.35.fc2.bias', 'esm.layers.35.final_layer_norm.weight', 'esm.layers.35.final_layer_norm.bias', 'esm.contact_head.regression.weight', 'esm.contact_head.regression.bias', 'esm.emb_layer_norm_after.weight', 'esm.emb_layer_norm_after.bias', 'esm.lm_head.weight', 'esm.lm_head.bias', 'esm.lm_head.dense.weight', 'esm.lm_head.dense.bias', 'esm.lm_head.layer_norm.weight', 'esm.lm_head.layer_norm.bias'], unexpected_keys=['positional_encoding._float_tensor', 'trunk.structure_module.default_frames', 'trunk.structure_module.group_idx', 'trunk.structure_module.atom_mask', 'trunk.structure_module.lit_positions'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESM_FOLD_MODEL_NAME = \"esm_fold_v1\"\n",
    "\n",
    "esm_fold_data = torch.load(f'../model/{ESM_FOLD_MODEL_NAME}.pt', \n",
    "                        mmap=True, weights_only=False)\n",
    "\n",
    "cfg = esm_fold_data[\"cfg\"][\"model\"]\n",
    "model_state = esm_fold_data[\"model\"]\n",
    "model = ESMFold(esm, alphabet, esmfold_config=cfg)\n",
    "\n",
    "expected_keys = set(model.state_dict().keys())\n",
    "found_keys = set(model_state.keys())\n",
    "\n",
    "missing_essential_keys = []\n",
    "for missing_key in expected_keys - found_keys:\n",
    "    if not missing_key.startswith(\"esm.\"):\n",
    "        missing_essential_keys.append(missing_key)\n",
    "\n",
    "if missing_essential_keys:\n",
    "    print(\"MISSING ESSENTIAL KEYS\")\n",
    "    raise RuntimeError(f\"Keys '{', '.join(missing_essential_keys)}' are missing.\")\n",
    "\n",
    "model.load_state_dict(model_state, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d5397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 100\n",
    "\n",
    "# Main amino acid sequence tensor - indices for amino acids\n",
    "# Based on the docstring, these should match openfold.np.residue_constants.restype_order_with_x\n",
    "# Typically amino acid indices range from 0-20 (20 standard amino acids + unknown/mask)\n",
    "aa = torch.randint(0, 21, (batch_size, sequence_length))\n",
    "\n",
    "# Optional: create a mask (1 for valid positions, 0 for padding)\n",
    "mask = torch.ones(batch_size, sequence_length)\n",
    "\n",
    "# Optional: residue indices (if not provided, will be assumed contiguous)\n",
    "residx = torch.arange(sequence_length).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "# Optional: masking pattern for ESMFold\n",
    "masking_pattern = torch.zeros(batch_size, sequence_length)  # No masking\n",
    "# Or to randomly mask some positions:\n",
    "# masking_pattern = torch.bernoulli(torch.full((batch_size, sequence_length), 0.1))  # 10% masked\n",
    "\n",
    "# Number of recycles (defaults to 3 if None)\n",
    "num_recycles = 3\n",
    "\n",
    "# Call the forward function\n",
    "output = model.forward(\n",
    "    aa=aa,\n",
    "    mask=mask,\n",
    "    residx=residx,\n",
    "    masking_pattern=masking_pattern,\n",
    "    num_recycles=num_recycles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69018bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 100, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"frames\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein-folding-acceleration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
